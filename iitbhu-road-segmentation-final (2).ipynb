{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10066228,"sourceType":"datasetVersion","datasetId":6203817},{"sourceId":10066924,"sourceType":"datasetVersion","datasetId":6204349},{"sourceId":10079394,"sourceType":"datasetVersion","datasetId":6213636}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastseg --q\n!pip install torchinfo --q\n!pip install albumentations --q\n!pip install geffnet --q\n!pip install tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:32:33.965282Z","iopub.execute_input":"2024-12-02T12:32:33.966145Z","iopub.status.idle":"2024-12-02T12:33:16.334423Z","shell.execute_reply.started":"2024-12-02T12:32:33.966078Z","shell.execute_reply":"2024-12-02T12:33:16.333538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fastseg\nfrom fastseg import MobileV3Large\nfrom PIL import Image\nimport cv2 as cv\nimport albumentations as A\nimport tqdm\nfrom torchinfo import summary\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport os\nimport shutil\nimport random\nfrom torch.utils.data import Dataset , DataLoader\nimport torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:33:19.532117Z","iopub.execute_input":"2024-12-02T12:33:19.532832Z","iopub.status.idle":"2024-12-02T12:33:24.528563Z","shell.execute_reply.started":"2024-12-02T12:33:19.532797Z","shell.execute_reply":"2024-12-02T12:33:24.527688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the directories\njpeg_folder = '/kaggle/working/jpeg_images'  # For JPEG images\npng_folder = '/kaggle/working/png_masks'      # For PNG masks\n\n# Remove the directories if they exist\nif os.path.exists(jpeg_folder):\n    shutil.rmtree(jpeg_folder)  # Remove the jpeg_images directory and its contents\nif os.path.exists(png_folder):\n    shutil.rmtree(png_folder)  # Remove the png_masks directory and its contents\n\n# Now create new directories\nos.makedirs(jpeg_folder, exist_ok=True)\nos.makedirs(png_folder, exist_ok=True)\n\n# Define the original folder for copying files\noriginal_folder = '/kaggle/input/train3/1'  # Replace with your actual path\n\n# Get a sorted list of all files in the original folder\nfiles = sorted(os.listdir(original_folder))\n\n# Initialize a counter for numbering\ncounter = 1\n\n# Copy files in pairs (image and mask)\nfor i in range(0, len(files), 2):\n    # Get the image file\n    image_file = files[i]\n    image_path = os.path.join(original_folder, image_file)\n    \n    # Check if there's a corresponding mask file\n    if i + 1 < len(files):\n        mask_file = files[i + 1]\n        mask_path = os.path.join(original_folder, mask_file)\n        \n        # Copy the unannotated image to the JPEG folder if it's a JPEG file\n        if image_file.lower().endswith('.jpeg') or image_file.lower().endswith('.jpg'):\n            new_image_name = f\"image_{counter}.jpeg\"  # Create a new name with numbering\n            shutil.copy(image_path, os.path.join(jpeg_folder, new_image_name))\n        \n        # Copy the annotated mask to the PNG folder if it's a PNG file\n        if mask_file.lower().endswith('.png'):\n            new_mask_name = f\"mask_{counter}.png\"  # Create a new name with numbering\n            shutil.copy(mask_path, os.path.join(png_folder, new_mask_name))\n        else:\n            print(f\"Warning: Mask file '{mask_file}' is not a PNG file.\")\n    else:\n        print(f\"Warning: No mask found for image '{image_file}'.\")\n    \n    # Increment the counter after processing each pair\n    counter += 1\n\nprint(\"Files have been successfully copied and renamed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:34:12.009435Z","iopub.execute_input":"2024-12-02T12:34:12.010307Z","iopub.status.idle":"2024-12-02T12:34:50.991613Z","shell.execute_reply.started":"2024-12-02T12:34:12.010273Z","shell.execute_reply":"2024-12-02T12:34:50.990536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MobileV3Large.from_pretrained()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:15.494836Z","iopub.execute_input":"2024-12-02T12:35:15.495662Z","iopub.status.idle":"2024-12-02T12:35:18.551332Z","shell.execute_reply.started":"2024-12-02T12:35:15.495625Z","shell.execute_reply":"2024-12-02T12:35:18.550563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:22.434707Z","iopub.execute_input":"2024-12-02T12:35:22.435504Z","iopub.status.idle":"2024-12-02T12:35:22.443579Z","shell.execute_reply.started":"2024-12-02T12:35:22.435470Z","shell.execute_reply":"2024-12-02T12:35:22.442768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.conv_up3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:38.738457Z","iopub.execute_input":"2024-12-02T12:35:38.739164Z","iopub.status.idle":"2024-12-02T12:35:38.744503Z","shell.execute_reply.started":"2024-12-02T12:35:38.739125Z","shell.execute_reply":"2024-12-02T12:35:38.743709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.conv_up2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:39.865266Z","iopub.execute_input":"2024-12-02T12:35:39.865862Z","iopub.status.idle":"2024-12-02T12:35:39.871368Z","shell.execute_reply.started":"2024-12-02T12:35:39.865831Z","shell.execute_reply":"2024-12-02T12:35:39.870469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Add segmentation-specific layers (last and second-last layers)\nmodel.second_last = nn.Sequential(\n    nn.Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1)),\n    nn.ReLU(),\n    nn.BatchNorm2d(128),\n    nn.Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1)),\n    nn.ReLU(),\n    nn.BatchNorm2d(64)\n)\n\nmodel.last = nn.Sequential(\n    nn.Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)),\n    nn.ReLU(),\n    nn.BatchNorm2d(128),\n    nn.Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1)),\n    nn.ReLU(),\n    nn.BatchNorm2d(64),\n    nn.Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1)),\n    nn.ReLU(),\n    nn.BatchNorm2d(32),\n    nn.Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1)),\n    nn.ReLU(),\n    nn.BatchNorm2d(16),\n    nn.Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1)),\n    nn.ReLU(),\n    nn.BatchNorm2d(8),\n    nn.Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1)),\n    nn.ReLU(),\n    nn.BatchNorm2d(4),\n    nn.Conv2d(4, 1, kernel_size=(1, 1), stride=(1, 1))\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:41.668394Z","iopub.execute_input":"2024-12-02T12:35:41.669248Z","iopub.status.idle":"2024-12-02T12:35:41.680992Z","shell.execute_reply.started":"2024-12-02T12:35:41.669214Z","shell.execute_reply":"2024-12-02T12:35:41.680159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:43.530714Z","iopub.execute_input":"2024-12-02T12:35:43.531603Z","iopub.status.idle":"2024-12-02T12:35:43.539003Z","shell.execute_reply.started":"2024-12-02T12:35:43.531570Z","shell.execute_reply":"2024-12-02T12:35:43.538137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Freeze all layers in the model\nfor param in model.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:44.781704Z","iopub.execute_input":"2024-12-02T12:35:44.782520Z","iopub.status.idle":"2024-12-02T12:35:44.786959Z","shell.execute_reply.started":"2024-12-02T12:35:44.782488Z","shell.execute_reply":"2024-12-02T12:35:44.786036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count total trainable parameters\ndef count_trainable_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ntrainable_params = count_trainable_params(model)\nprint(f\"Total Trainable Parameters: {trainable_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:46.448822Z","iopub.execute_input":"2024-12-02T12:35:46.449524Z","iopub.status.idle":"2024-12-02T12:35:46.454990Z","shell.execute_reply.started":"2024-12-02T12:35:46.449492Z","shell.execute_reply":"2024-12-02T12:35:46.454056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unfreeze the last layer\nfor param in model.last.parameters():\n    param.requires_grad = True\n\n# Unfreeze the second-last layer\nfor param in model.second_last.parameters():\n    param.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:48.011638Z","iopub.execute_input":"2024-12-02T12:35:48.012432Z","iopub.status.idle":"2024-12-02T12:35:48.016526Z","shell.execute_reply.started":"2024-12-02T12:35:48.012399Z","shell.execute_reply":"2024-12-02T12:35:48.015605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count total trainable parameters\ndef count_trainable_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ntrainable_params = count_trainable_params(model)\nprint(f\"Total Trainable Parameters: {trainable_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:49.766069Z","iopub.execute_input":"2024-12-02T12:35:49.766926Z","iopub.status.idle":"2024-12-02T12:35:49.772369Z","shell.execute_reply.started":"2024-12-02T12:35:49.766893Z","shell.execute_reply":"2024-12-02T12:35:49.771499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unfreeze the conv_up2 layer\nfor param in model.conv_up2.parameters():\n    param.requires_grad = True\n\n# Unfreeze the conv_up3 layer\nfor param in model.conv_up3.parameters():\n    param.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:51.575298Z","iopub.execute_input":"2024-12-02T12:35:51.575676Z","iopub.status.idle":"2024-12-02T12:35:51.580286Z","shell.execute_reply.started":"2024-12-02T12:35:51.575646Z","shell.execute_reply":"2024-12-02T12:35:51.579268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count total trainable parameters\ndef count_trainable_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ntrainable_params = count_trainable_params(model)\nprint(f\"Total Trainable Parameters: {trainable_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:53.266928Z","iopub.execute_input":"2024-12-02T12:35:53.267252Z","iopub.status.idle":"2024-12-02T12:35:53.273004Z","shell.execute_reply.started":"2024-12-02T12:35:53.267225Z","shell.execute_reply":"2024-12-02T12:35:53.272155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Assuming 'model' is your MobileNetV3 model\n# Accessing block4 from the model\nblock4 = model.trunk.block4\n\n# Unfreeze all layers in block4\nfor module in block4:\n    for param in module.parameters():\n        param.requires_grad = True\n\n# Check if the parameters in block4 are unfrozen\nfor module in block4:\n    for param in module.parameters():\n        print(f'Parameter {param} requires grad: {param.requires_grad}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:55.733236Z","iopub.execute_input":"2024-12-02T12:35:55.733921Z","iopub.status.idle":"2024-12-02T12:35:55.844004Z","shell.execute_reply.started":"2024-12-02T12:35:55.733891Z","shell.execute_reply":"2024-12-02T12:35:55.843084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count total trainable parameters\ndef count_trainable_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ntrainable_params = count_trainable_params(model)\nprint(f\"Total Trainable Parameters: {trainable_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:35:58.713510Z","iopub.execute_input":"2024-12-02T12:35:58.714322Z","iopub.status.idle":"2024-12-02T12:35:58.720068Z","shell.execute_reply.started":"2024-12-02T12:35:58.714289Z","shell.execute_reply":"2024-12-02T12:35:58.719153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:39:49.539815Z","iopub.execute_input":"2024-12-02T12:39:49.540392Z","iopub.status.idle":"2024-12-02T12:39:49.544254Z","shell.execute_reply.started":"2024-12-02T12:39:49.540356Z","shell.execute_reply":"2024-12-02T12:39:49.543259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:40:02.465344Z","iopub.execute_input":"2024-12-02T12:40:02.465649Z","iopub.status.idle":"2024-12-02T12:40:02.486995Z","shell.execute_reply.started":"2024-12-02T12:40:02.465624Z","shell.execute_reply":"2024-12-02T12:40:02.486143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset_Builder(Dataset):\n    def __init__(self,img_list ,mask_list ):\n        super().__init__()\n        self.img_list = img_list\n        self.mask_list = mask_list\n        self.augmentation_transform = A.Compose([\n#     A.HorizontalFlip(p=0.5),  # Randomly flip images horizontally\n    A.Rotate(limit=(-180, 180) , p=0.5 ),\n#     A.Perspective(scale=(0.05, 0.1), p=0.5)\n#     A.RandomResizedCrop(height=512, width=512, scale=(0.8, 1.0), p=0.5),\n\n        ])\n        \n        self.augmentation_transform_img = A.Compose([\n                                A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2, p=0.5),  # Vary color conditions\n                                A.GaussianBlur(blur_limit=(3, 5), p=0.3),  # Simulate Gaussian noise\n                                A.MotionBlur(blur_limit=7, p=0.4),  # Apply motion blur with a probability of 40%\n                                A.RandomSunFlare(flare_roi=(0.3, 0.3, 1, 0.6), angle_lower=0.5, src_radius=100, p=0.3),\n                                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0),\n                                ToTensorV2()])\n        \n        self.augmentation_transform_mask = A.Compose([\n            ToTensorV2()\n        ])\n        \n    def __len__(self):\n        return len(self.img_list)\n    \n    def __getitem__(self,idx):\n        mask = cv.imread('/kaggle/working/png_masks/'+ self.mask_list[idx])\n        img = cv.imread('/kaggle/working/jpeg_images/'+self.img_list[idx])\n       \n        \n        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n        mask = cv.cvtColor(mask, cv.COLOR_BGR2RGB)\n#         print(mask)\n        img = cv.resize(img, (512, 512))\n        mask = cv.resize(mask, (512, 512))\n        augmented = self.augmentation_transform(image=img, mask=mask)\n        img = augmented['image']\n        mask = augmented['mask']\n        img = self.augmentation_transform_img(image=img)['image']\n        mask = self.augmentation_transform_mask(image=mask)['image']\n\n        return img,mask\n\nIMG_DIR = \"/kaggle/working/jpeg_images\"\nMASK_DIR = \"/kaggle/working/png_masks\"\n\nimagedir = os.listdir(IMG_DIR)\nmaskdir = os.listdir(MASK_DIR)\n\nimagedir.sort()\nmaskdir.sort()\n\n# Create dataset and dataloaders\ndataset = Dataset_Builder(img_list=imagedir, mask_list=maskdir)\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n# Dataloaders for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\nprint(\"Dataloaders are ready for training and validation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:38:36.030774Z","iopub.execute_input":"2024-12-02T12:38:36.031140Z","iopub.status.idle":"2024-12-02T12:38:36.056283Z","shell.execute_reply.started":"2024-12-02T12:38:36.031080Z","shell.execute_reply":"2024-12-02T12:38:36.055444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming train_dataset[0] returns (image, mask)\nimage, mask = train_dataset[1]  # Replace with your dataset access method\n\n# Plotting the image\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.imshow(image.permute(1, 2, 0))  # If using PyTorch, permute to (H, W, C)\nplt.title('Image')\nplt.axis('off')\n\n# Plotting the mask\nplt.subplot(1, 2, 2)\nplt.imshow(mask.permute(1, 2, 0), cmap='gray')  # Use gray colormap for mask\nplt.title('Mask')\nplt.axis('off')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:38:40.138831Z","iopub.execute_input":"2024-12-02T12:38:40.139700Z","iopub.status.idle":"2024-12-02T12:38:40.551001Z","shell.execute_reply.started":"2024-12-02T12:38:40.139661Z","shell.execute_reply":"2024-12-02T12:38:40.550027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda'\nnum_epochs = 25\noptimizer = torch.optim.AdamW(params=model.parameters())\ncriterion = torch.nn.BCEWithLogitsLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:05:27.038757Z","iopub.execute_input":"2024-12-02T13:05:27.039602Z","iopub.status.idle":"2024-12-02T13:05:27.047114Z","shell.execute_reply.started":"2024-12-02T13:05:27.039554Z","shell.execute_reply":"2024-12-02T13:05:27.046300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:38:51.808891Z","iopub.execute_input":"2024-12-02T12:38:51.809510Z","iopub.status.idle":"2024-12-02T12:38:51.853082Z","shell.execute_reply.started":"2024-12-02T12:38:51.809476Z","shell.execute_reply":"2024-12-02T12:38:51.852141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def IOU(pred_image, ground_truth):\n    total_iou = 0\n    batch_size = pred_image.shape[0]\n\n    for pred, ground in zip(pred_image, ground_truth):\n        pred = pred.flatten()\n        ground = ground.flatten()\n        \n        # Calculate intersection\n        intersection = np.sum((pred == 1) & (ground == 1)).astype(int)\n        \n        # Calculate union\n        union = np.sum((pred == 1) | (ground == 1)).astype(int)\n        \n        # Calculate IoU\n        if union == 0:\n            # If both are empty, define IoU as 1 (perfect match)\n            iou = 1\n        else:\n            iou = intersection / union\n        \n        total_iou += iou  # Accumulate IoU for the current batch\n\n    return total_iou / batch_size if batch_size > 0 else 0  # Return the average IoU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:38:58.283344Z","iopub.execute_input":"2024-12-02T12:38:58.284058Z","iopub.status.idle":"2024-12-02T12:38:58.289590Z","shell.execute_reply.started":"2024-12-02T12:38:58.284024Z","shell.execute_reply":"2024-12-02T12:38:58.288614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm  # Import tqdm for the progress bar\n\n\nsave_path = \"saved_models/\"\n\nos.makedirs(save_path, exist_ok=True)\nval_epoch_loss_list = []\nval_batch_loss_list = []\ntrain_epoch_loss_list = []\ntrain_batch_loss_list = []\nIOU_Score_list = []\n\nfor epoch in range(num_epochs):\n    \n    train_epoch_loss = 0\n    val_epoch_loss = 0\n    batch = 0\n\n    # Wrap the DataLoader with tqdm for a progress bar\n    with tqdm(train_loader, unit=\"batch\") as tepoch:\n        tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n        model.train()\n        for img, mask in tepoch:\n            batch += 1\n            \n            img = img.to(torch.float32).to(device)\n            mask = mask[:, 0:1, :, :]\n            mask = mask.to(torch.float32)\n#             print(img.shape)\n#             print(mask.shape)\n            y_pred = model(img)\n#             print('done')\n            loss = criterion(y_pred.flatten() , mask.to(device).flatten())\n\n            with torch.no_grad():\n                train_batch_loss_list.append(loss.item())  # Convert loss to Python float\n                train_epoch_loss += loss.item()\n                tepoch.set_postfix(Batch_Loss=loss.item(), Batch_no=batch)\n\n            \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        # Log the average loss for the epoch at the end of the epoch\n        with torch.no_grad():\n            avg_loss = train_epoch_loss / len(train_loader)\n            print(f\"Epoch {epoch+1}/{num_epochs} completed. Avg Train Loss: {avg_loss:.4f}\")\n            train_epoch_loss_list.append(avg_loss)\n    \n#     with torch.no_grad():\n#         predict_image(model, img, device)\n    batch = 0        \n    with tqdm(val_loader, unit=\"batch\") as tepoch:\n        tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n        model.eval()\n        with torch.no_grad():\n            for img, mask in tepoch:\n                batch += 1\n            \n                img = img.to(torch.float32).to(device)\n                mask = mask.float()\n\n                y_pred = model(img)\n                mask = mask[:, 0:1, :, :]\n#                 print(y_pred[0],mask)\n                loss = criterion(y_pred, mask.to(device))\n\n            with torch.no_grad():\n                val_batch_loss_list.append(loss.item())  # Convert loss to Python float\n                val_epoch_loss += loss.item()\n                tepoch.set_postfix(Batch_Loss=loss.item(), Batch_no=batch)\n            \n            with torch.no_grad():\n                avg_loss = val_epoch_loss / len(val_loader)\n                print(f\"Epoch {epoch+1}/{num_epochs} completed. Avg Validation Loss: {avg_loss:.4f}\")\n                val_epoch_loss_list.append(avg_loss)\n                \n\n            \n            \n    with torch.no_grad():\n        Avg_IOU_Score = 0\n        for img, mask in train_loader:\n        \n          \n            img = img.float().to(device)\n            mask = mask[:, 0:1, :, :]\n            mask = mask.float().to('cpu').numpy()\n\n            y_pred = model(img)\n            y_pred = torch.nn.functional.sigmoid(y_pred).to('cpu').numpy()\n            y_pred_binary = np.zeros(y_pred.shape)\n            y_pred_binary[y_pred>=0.5] = 1\n        \n            IOU_Score = IOU(mask , y_pred_binary)\n            Avg_IOU_Score += IOU_Score\n        print(f'Avg_IOU_Score :: {Avg_IOU_Score/len(train_loader)}') \n        IOU_Score_list.append(Avg_IOU_Score/len(train_loader))\n        \n\n    with torch.no_grad():    \n        # Save the model after each epoch\n        model_save_name = f\"{save_path}last_layer+aug{epoch+1}.pth\"\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': avg_loss\n        }, model_save_name)\n\n        print(f\"Model saved as {model_save_name}\")\n        print(\"***************************************************************************\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:05:40.674722Z","iopub.execute_input":"2024-12-02T13:05:40.675154Z","iopub.status.idle":"2024-12-02T15:18:20.295766Z","shell.execute_reply.started":"2024-12-02T13:05:40.675113Z","shell.execute_reply":"2024-12-02T15:18:20.294405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Function to load an image and get the model prediction\ndef predict_image(model, image_path, device):\n    # Load the image\n    image = Image.open(image_path).convert(\"RGB\")  # Ensure it's in RGB format\n    \n    # Define the necessary transformations\n    preprocess = transforms.Compose([\n        transforms.Resize((512, 512)),  # Resize to match model input size\n        transforms.ToTensor(),  # Convert image to tensor\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Preprocess the image\n    input_image = preprocess(image).unsqueeze(0)  # Add batch dimension\n\n    # Move the image to the specified device\n    input_image = input_image.to(device)\n\n    # Make the prediction\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        output = model(input_image)\n\n    # If the output is a single channel (e.g., for segmentation), apply sigmoid\n    if output.shape[1] == 1:  # Assuming the model outputs a single channel\n        output = torch.sigmoid(output)  # Convert logits to probabilities\n        output = (output > 0.5).float()  # Binarize the output\n\n    # Squeeze the output to remove batch and channel dimensions\n    output_image = output.squeeze().cpu().numpy()  # Convert to NumPy array\n\n    # Plot the original image and the prediction\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.title(\"Original Image\")\n    plt.imshow(image)\n    plt.axis(\"off\")\n\n    plt.subplot(1, 2, 2)\n    plt.title(\"Model Prediction\")\n    plt.imshow(output_image, cmap='gray')  # Change cmap based on output\n    plt.axis(\"off\")\n\n    plt.show()\n\n# Example usage\nimage_path = '/kaggle/input/test-data/test_image.jpg'\npredict_image(model, image_path, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:00:05.367582Z","iopub.execute_input":"2024-12-02T16:00:05.368279Z","iopub.status.idle":"2024-12-02T16:00:05.881982Z","shell.execute_reply.started":"2024-12-02T16:00:05.368245Z","shell.execute_reply":"2024-12-02T16:00:05.881152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\n# Load the video\ninput_video_path = '/kaggle/input/test-video-3/test_video.mp4'\noutput_video_path = '/kaggle/working/output_video.mp4'\n\ncap = cv2.VideoCapture(input_video_path)\n\n# Define video codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = 512  # Resize width\nframe_height = 512  # Resize height\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\n# Define a transform for PyTorch model\ntorch_transform = transforms.Compose([\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Only normalization step\n])\n\nmodel.eval()  # Set to evaluation mode\n\nframe_count = 0  # Track the number of processed frames\n\n# Loop through video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n\n    if not ret:\n        print(\"Error: Could not read frame.\")\n        break\n\n    # Resize frame for model input to 512x512\n    frame_resized = cv2.resize(frame, (frame_width, frame_height))\n\n    # Convert to NumPy array and prepare for PyTorch model\n    frame_resized_np = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    frame_resized_torch = torch.from_numpy(frame_resized_np).permute(2, 0, 1).float() / 255.0\n    frame_resized_torch = torch_transform(frame_resized_torch)\n\n    # Add batch dimension and predict the mask\n    with torch.no_grad():\n        mask = model(frame_resized_torch.unsqueeze(0).cuda())[0]  # Forward pass\n\n    # Squeeze the mask to remove the batch dimension\n    mask = mask.squeeze(0).cpu().numpy()  # Now it should be (512, 512)\n\n    # Check the shape of the mask after squeezing\n    if len(mask.shape) != 2:\n        print(f\"Error: Expected single-channel mask, but got shape: {mask.shape}\")\n        break\n\n    # Apply threshold\n    mask = np.where(mask > 0.8, 1, 0).astype(np.uint8)  # Assuming the mask is in [0, 1]\n\n    # Resize mask to ensure it matches frame dimensions\n    mask_resized = cv2.resize(mask, (frame_width, frame_height))\n\n    # Create an all-red overlay where mask_resized is 1\n    mask_red = np.zeros_like(frame_resized)  # Create a blank image with the same dimensions as the frame\n    mask_red[:, :, 2] = mask_resized * 255  # Set red channel to 255 where mask is 1\n\n    # Overlay red mask on the original frame\n    overlayed_frame = cv2.addWeighted(frame_resized, 1, mask_red, 0.5, 0)\n\n    # Write the processed frame to the output video\n    out.write(overlayed_frame)\n\n    frame_count += 1  # Increment the frame count\n    if frame_count % 200 == 0:\n        print(f\"Processed frame: {frame_count}\")  # Print progress\n\n# Release the video capture and writer objects\ncap.release()\nout.release()\n\n# Close all OpenCV windows (commenting out as this may cause issues in your environment)\n# cv2.destroyAllWindows()\n\nprint(f\"Video processing completed. Output saved at {output_video_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:21:54.932607Z","iopub.execute_input":"2024-12-02T16:21:54.932959Z","iopub.status.idle":"2024-12-02T16:22:59.689539Z","shell.execute_reply.started":"2024-12-02T16:21:54.932926Z","shell.execute_reply":"2024-12-02T16:22:59.688653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\n# Input and output paths\ninput_video_path = '/kaggle/working/output_video.mp4'\noutput_video_path = '/kaggle/working/output_video_erosion.mp4'\n\n# Video properties\ncap = cv2.VideoCapture(input_video_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = 512\nframe_height = 512\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\n# PyTorch transforms\ntorch_transform = transforms.Compose([\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Parameters for erosion and filtering\nmin_area = 500  # Minimum area to keep regions\nkernel_size = 5  # Erosion kernel size\n\n# Function to apply region filtering based on area\ndef filter_regions_by_area(binary_mask, min_area):\n    # Find contours in the binary mask\n    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    filtered_mask = np.zeros_like(binary_mask)\n\n    for contour in contours:\n        if cv2.contourArea(contour) > min_area:\n            cv2.drawContours(filtered_mask, [contour], -1, 255, thickness=cv2.FILLED)\n    return filtered_mask\n\n# Loop through video frames\nframe_count = 0\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        print(\"End of video or error reading frame.\")\n        break\n\n    # Resize frame\n    resized_frame = cv2.resize(frame, (frame_width, frame_height))\n\n    # Convert frame to RGB and tensor for model input\n    rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n    frame_tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0\n    frame_tensor = torch_transform(frame_tensor).unsqueeze(0).cuda()\n\n    # Model prediction\n    with torch.no_grad():\n        mask = model(frame_tensor)[0].squeeze(0).cpu().numpy()  # (512, 512)\n\n    # Threshold and resize mask\n    binary_mask = (mask > 0.8).astype(np.uint8)  # Binary segmentation mask\n    binary_mask_resized = cv2.resize(binary_mask, (frame_width, frame_height))\n\n    # Erosion for noise removal\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n    eroded_mask = cv2.erode(binary_mask_resized, kernel, iterations=1)\n\n    # Region filtering\n    filtered_mask = filter_regions_by_area(eroded_mask, min_area)\n\n    # Depth-based or contextual filtering (optional, placeholder for depth/context constraints)\n    # depth_filtered_mask = apply_depth_constraints(filtered_mask, depth_map)  # Integrate if depth is available\n\n    # Highlight the segmented road in red on the original frame\n    overlay = np.zeros_like(resized_frame)\n    overlay[:, :, 2] = filtered_mask * 255  # Red channel\n    overlayed_frame = cv2.addWeighted(resized_frame, 1, overlay, 0.5, 0)\n\n    # Write the frame\n    out.write(overlayed_frame)\n\n    frame_count += 1\n    if frame_count % 100 == 0:\n        print(f\"Processed {frame_count} frames.\")\n\n# Release resources\ncap.release()\nout.release()\nprint(f\"Processed video saved at {output_video_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:17:28.986505Z","iopub.execute_input":"2024-12-02T16:17:28.987237Z","iopub.status.idle":"2024-12-02T16:20:09.630630Z","shell.execute_reply.started":"2024-12-02T16:17:28.987201Z","shell.execute_reply":"2024-12-02T16:20:09.629646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}